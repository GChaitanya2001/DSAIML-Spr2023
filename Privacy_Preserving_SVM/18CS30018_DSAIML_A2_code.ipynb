{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **NAME**    : Gajula Sai Chaitanya\n",
        "## **ROLL NO** : 18CS30018"
      ],
      "metadata": {
        "id": "_8QnHgGXha4y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMAFw1JkjMS5",
        "outputId": "2ccc0a21-fdc5-4505-dff1-c31031b99ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/data61/python-paillier.git\n",
            "  Cloning https://github.com/data61/python-paillier.git to /tmp/pip-req-build-71bi6qc0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/data61/python-paillier.git /tmp/pip-req-build-71bi6qc0\n",
            "  Resolved https://github.com/data61/python-paillier.git to commit 7d9911eb03c3c2d64399bc15405feb5e628379d1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/data61/python-paillier.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part - 1: Running the codes and recording timing for Federated Learning"
      ],
      "metadata": {
        "id": "GQw95Y49AQ1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3.4\n",
        "import math\n",
        "\n",
        "import phe.encoding\n",
        "from phe import paillier\n",
        "\n",
        "\n",
        "class ExampleEncodedNumber(phe.encoding.EncodedNumber):\n",
        "    BASE = 64\n",
        "    LOG2_BASE = math.log(BASE, 2)\n",
        "\n",
        "\n",
        "print(\"Generating paillier keypair\")\n",
        "public_key, private_key = paillier.generate_paillier_keypair()\n",
        "\n",
        "\n",
        "def encode_and_encrypt_example():\n",
        "    print(\"Encoding a large positive number. With a BASE {} encoding scheme\".format(ExampleEncodedNumber.BASE))\n",
        "    encoded = ExampleEncodedNumber.encode(public_key, 2.1 ** 20)\n",
        "    print(\"Checking that decoding gives the same number...\")\n",
        "    assert 2.1 ** 20 == encoded.decode()\n",
        "\n",
        "    print(\"Encrypting the encoded number\")\n",
        "    encrypted = public_key.encrypt(encoded)\n",
        "\n",
        "    print(\"Decrypting...\")\n",
        "    decrypted_but_encoded = \\\n",
        "        private_key.decrypt_encoded(encrypted, ExampleEncodedNumber)\n",
        "\n",
        "    print(\"Checking the decrypted number is what we started with\")\n",
        "    assert abs(2.1 ** 20 - decrypted_but_encoded.decode()) < 1e-12\n",
        "\n",
        "\n",
        "def math_example():\n",
        "    print(\"Encoding two large positive numbers. BASE={}\".format(ExampleEncodedNumber.BASE))\n",
        "\n",
        "    a = 102545 + (64 ** 8)\n",
        "    b = 123 + (8 ** 20)\n",
        "\n",
        "    encoded_a = ExampleEncodedNumber.encode(public_key, a)\n",
        "    encoded_b = ExampleEncodedNumber.encode(public_key, b)\n",
        "\n",
        "    print(\"Checking that decoding gives the same number...\")\n",
        "    assert a == encoded_a.decode()\n",
        "    assert b == encoded_b.decode()\n",
        "\n",
        "    print(\"Encrypting the encoded numbers\")\n",
        "    encrypted_a = public_key.encrypt(encoded_a)\n",
        "    encrypted_b = public_key.encrypt(encoded_b)\n",
        "\n",
        "    print(\"Adding the encrypted numbers\")\n",
        "    encrypted_c = encrypted_a + encrypted_b\n",
        "\n",
        "    print(\"Decrypting the one encrypted sum\")\n",
        "    decrypted_but_encoded = \\\n",
        "        private_key.decrypt_encoded(encrypted_c, ExampleEncodedNumber)\n",
        "\n",
        "    print(\"Checking the decrypted number is what we started with\")\n",
        "\n",
        "    print(\"Decrypted: {}\".format(decrypted_but_encoded.decode()))\n",
        "    assert abs((a + b) - decrypted_but_encoded.decode()) < 1e-15\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    encode_and_encrypt_example()\n",
        "\n",
        "    math_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiEIARvzjoal",
        "outputId": "a2886691-efbc-4899-be6d-a8d72e775824"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating paillier keypair\n",
            "Encoding a large positive number. With a BASE 64 encoding scheme\n",
            "Checking that decoding gives the same number...\n",
            "Encrypting the encoded number\n",
            "Decrypting...\n",
            "Checking the decrypted number is what we started with\n",
            "Encoding two large positive numbers. BASE=64\n",
            "Checking that decoding gives the same number...\n",
            "Encrypting the encoded numbers\n",
            "Adding the encrypted numbers\n",
            "Decrypting the one encrypted sum\n",
            "Checking the decrypted number is what we started with\n",
            "Decrypted: 1153202979583660300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This example involves learning using sensitive medical data from multiple hospitals\n",
        "to predict diabetes progression in patients. The data is a standard dataset from\n",
        "sklearn[1].\n",
        "\n",
        "Recorded variables are:\n",
        "- age,\n",
        "- gender,\n",
        "- body mass index,\n",
        "- average blood pressure,\n",
        "- and six blood serum measurements.\n",
        "\n",
        "The target variable is a quantitative measure of the disease progression.\n",
        "Since this measure is continuous, we solve the problem using linear regression.\n",
        "\n",
        "The patients' data is split between 3 hospitals, all sharing the same features\n",
        "but different entities. We refer to this scenario as horizontally partitioned.\n",
        "\n",
        "The objective is to make use of the whole (virtual) training set to improve\n",
        "upon the model that can be trained locally at each hospital.\n",
        "\n",
        "50 patients will be kept as a test set and not used for training.\n",
        "\n",
        "An additional agent is the 'server' who facilitates the information exchange\n",
        "among the hospitals under the following privacy constraints:\n",
        "\n",
        "1) The individual patient's record at each hospital cannot leave the premises,\n",
        "   not even in encrypted form.\n",
        "2) Information derived (read: gradients) from any hospital's dataset\n",
        "   cannot be shared, unless it is first encrypted.\n",
        "3) None of the parties (hospitals AND server) should be able to infer WHERE\n",
        "   (in which hospital) a patient in the training set has been treated.\n",
        "\n",
        "Note that we do not protect from inferring IF a particular patient's data\n",
        "has been used during learning. Differential privacy could be used on top of\n",
        "our protocol for addressing the problem. For simplicity, we do not discuss\n",
        "it in this example.\n",
        "\n",
        "In this example linear regression is solved by gradient descent. The server\n",
        "creates a paillier public/private keypair and does not share the private key.\n",
        "The hospital clients are given the public key. The protocol works as follows.\n",
        "Until convergence: hospital 1 computes its gradient, encrypts it and sends it\n",
        "to hospital 2; hospital 2 computes its gradient, encrypts and sums it to\n",
        "hospital 1's; hospital 3 does the same and passes the overall sum to the\n",
        "server. The server obtains the gradient of the whole (virtual) training set;\n",
        "decrypts it and sends the gradient back - in the clear - to every client.\n",
        "The clients then update their respective local models.\n",
        "\n",
        "From the learning viewpoint, notice that we are NOT assuming that each\n",
        "hospital sees an unbiased sample from the same patients' distribution:\n",
        "hospitals could be geographically very distant or serve a diverse population.\n",
        "We simulate this condition by sampling patients NOT uniformly at random,\n",
        "but in a biased fashion.\n",
        "The test set is instead an unbiased sample from the overall distribution.\n",
        "\n",
        "From the security viewpoint, we consider all parties to be \"honest but curious\".\n",
        "Even by seeing the aggregated gradient in the clear, no participant can pinpoint\n",
        "where patients' data originated. This is true if this RING protocol is run by\n",
        "at least 3 clients, which prevents reconstruction of each others' gradients\n",
        "by simple difference.\n",
        "\n",
        "This example was inspired by Google's work on secure protocols for federated\n",
        "learning[2].\n",
        "\n",
        "[1]: http://scikit-learn.org/stable/datasets/index.html#diabetes-dataset\n",
        "[2]: https://research.googleblog.com/2017/04/federated-learning-collaborative.html\n",
        "\n",
        "Dependencies: numpy, sklearn\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import phe as paillier\n",
        "\n",
        "seed = 43\n",
        "np.random.seed(seed)\n",
        "import time\n",
        "\n",
        "\n",
        "def get_data(n_clients):\n",
        "    \"\"\"\n",
        "    Import the dataset via sklearn, shuffle and split train/test.\n",
        "    Return training, target lists for `n_clients` and a holdout test set\n",
        "    \"\"\"\n",
        "    print(\"Loading data\")\n",
        "    diabetes = load_diabetes()\n",
        "    y = diabetes.target\n",
        "    X = diabetes.data\n",
        "    # Add constant to emulate intercept\n",
        "    X = np.c_[X, np.ones(X.shape[0])]\n",
        "\n",
        "    # The features are already preprocessed\n",
        "    # Shuffle\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    X, y = X[perm, :], y[perm]\n",
        "\n",
        "    # Select test at random\n",
        "    test_size = 50\n",
        "    test_idx = np.random.choice(X.shape[0], size=test_size, replace=False)\n",
        "    train_idx = np.ones(X.shape[0], dtype=bool)\n",
        "    train_idx[test_idx] = False\n",
        "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "    X_train, y_train = X[train_idx, :], y[train_idx]\n",
        "\n",
        "    # Split train among multiple clients.\n",
        "    # The selection is not at random. We simulate the fact that each client\n",
        "    # sees a potentially very different sample of patients.\n",
        "    X, y = [], []\n",
        "    step = int(X_train.shape[0] / n_clients)\n",
        "    for c in range(n_clients):\n",
        "        X.append(X_train[step * c: step * (c + 1), :])\n",
        "        y.append(y_train[step * c: step * (c + 1)])\n",
        "\n",
        "    return X, y, X_test, y_test\n",
        "\n",
        "\n",
        "def mean_square_error(y_pred, y):\n",
        "    \"\"\" 1/m * \\sum_{i=1..m} (y_pred_i - y_i)^2 \"\"\"\n",
        "    return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "\n",
        "def encrypt_vector(public_key, x):\n",
        "    return [public_key.encrypt(i) for i in x]\n",
        "\n",
        "\n",
        "def decrypt_vector(private_key, x):\n",
        "    return np.array([private_key.decrypt(i) for i in x])\n",
        "\n",
        "\n",
        "def sum_encrypted_vectors(x, y):\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError('Encrypted vectors must have the same size')\n",
        "    return [x[i] + y[i] for i in range(len(x))]\n",
        "\n",
        "\n",
        "class Server:\n",
        "    \"\"\"Private key holder. Decrypts the average gradient\"\"\"\n",
        "\n",
        "    def __init__(self, key_length):\n",
        "         keypair = paillier.generate_paillier_keypair(n_length=key_length)\n",
        "         self.pubkey, self.privkey = keypair\n",
        "\n",
        "    def decrypt_aggregate(self, input_model, n_clients):\n",
        "        return decrypt_vector(self.privkey, input_model) / n_clients\n",
        "\n",
        "\n",
        "class Client:\n",
        "    \"\"\"Runs linear regression with local data or by gradient steps,\n",
        "    where gradient can be passed in.\n",
        "\n",
        "    Using public key can encrypt locally computed gradients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name, X, y, pubkey):\n",
        "        self.name = name\n",
        "        self.pubkey = pubkey\n",
        "        self.X, self.y = X, y\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "\n",
        "    def fit(self, n_iter, eta=0.01):\n",
        "        \"\"\"Linear regression for n_iter\"\"\"\n",
        "        for _ in range(n_iter):\n",
        "            gradient = self.compute_gradient()\n",
        "            self.gradient_step(gradient, eta)\n",
        "\n",
        "    def gradient_step(self, gradient, eta=0.01):\n",
        "        \"\"\"Update the model with the given gradient\"\"\"\n",
        "        self.weights -= eta * gradient\n",
        "\n",
        "    def compute_gradient(self):\n",
        "        \"\"\"Compute the gradient of the current model using the training set\n",
        "        \"\"\"\n",
        "        delta = self.predict(self.X) - self.y\n",
        "        return delta.dot(self.X) / len(self.X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Score test data\"\"\"\n",
        "        return X.dot(self.weights)\n",
        "\n",
        "    def encrypted_gradient(self, sum_to=None):\n",
        "        \"\"\"Compute and encrypt gradient.\n",
        "\n",
        "        When `sum_to` is given, sum the encrypted gradient to it, assumed\n",
        "        to be another vector of the same size\n",
        "        \"\"\"\n",
        "        gradient = self.compute_gradient()\n",
        "        encrypted_gradient = encrypt_vector(self.pubkey, gradient)\n",
        "\n",
        "        if sum_to is not None:\n",
        "            return sum_encrypted_vectors(sum_to, encrypted_gradient)\n",
        "        else:\n",
        "            return encrypted_gradient\n",
        "\n",
        "\n",
        "def federated_learning(X, y, X_test, y_test, config):\n",
        "    n_clients = config['n_clients']\n",
        "    n_iter = config['n_iter']\n",
        "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
        "\n",
        "    # Instantiate the server and generate private and public keys\n",
        "    # NOTE: using smaller keys sizes wouldn't be cryptographically safe\n",
        "    server = Server(key_length=config['key_length'])\n",
        "\n",
        "    # Instantiate the clients.\n",
        "    # Each client gets the public key at creation and its own local dataset\n",
        "    clients = []\n",
        "    for i in range(n_clients):\n",
        "        clients.append(Client(names[i], X[i], y[i], server.pubkey))\n",
        "\n",
        "    # The federated learning with gradient descent\n",
        "    print('Running distributed gradient aggregation for {:d} iterations'\n",
        "          .format(n_iter))\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        # Compute gradients, encrypt and aggregate\n",
        "        encrypt_aggr = clients[0].encrypted_gradient(sum_to=None)\n",
        "        for c in clients[1:]:\n",
        "            encrypt_aggr = c.encrypted_gradient(sum_to=encrypt_aggr)\n",
        "\n",
        "        # Send aggregate to server and decrypt it\n",
        "        aggr = server.decrypt_aggregate(encrypt_aggr, n_clients)\n",
        "\n",
        "        # Take gradient steps\n",
        "        for c in clients:\n",
        "            c.gradient_step(aggr, config['eta'])\n",
        "\n",
        "    print('Error (MSE) that each client gets after running the protocol:')\n",
        "    start = time.time()\n",
        "    for c in clients:\n",
        "        y_pred = c.predict(X_test)\n",
        "        mse = mean_square_error(y_pred, y_test)\n",
        "        print('{:s}:\\t{:.2f}'.format(c.name, mse))\n",
        "    end = time.time()\n",
        "    print(\"Time required for Single Prediction (Local learning): \", (end-start)/(X_test.shape[0]))\n",
        "\n",
        "\n",
        "def local_learning(X, y, X_test, y_test, config):\n",
        "    n_clients = config['n_clients']\n",
        "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
        "\n",
        "    # Instantiate the clients.\n",
        "    # Each client gets the public key at creation and its own local dataset\n",
        "    clients = []\n",
        "    for i in range(n_clients):\n",
        "        clients.append(Client(names[i], X[i], y[i], None))\n",
        "\n",
        "    # Each client trains a linear regressor on its own data\n",
        "    print('Error (MSE) that each client gets on test set by '\n",
        "          'training only on own local data:')\n",
        "    start = time.time()\n",
        "    for c in clients:\n",
        "        c.fit(config['n_iter'], config['eta'])\n",
        "        y_pred = c.predict(X_test)\n",
        "        mse = mean_square_error(y_pred, y_test)\n",
        "        print('{:s}:\\t{:.2f}'.format(c.name, mse))\n",
        "    end = time.time()\n",
        "    print(\"Time required for Single Prediction (Local learning): \", (end-start)/(X_test.shape[0]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'n_clients': 5,\n",
        "        'key_length': 1024,\n",
        "        'n_iter': 50,\n",
        "        'eta': 1.5,\n",
        "    }\n",
        "    # load data, train/test split and split training data between clients\n",
        "    X, y, X_test, y_test = get_data(n_clients=config['n_clients'])\n",
        "    # first each hospital learns a model on its respective dataset for comparison.\n",
        "    print(\"################################ LOCAL LEARNING (Linear Regression) #############################\")\n",
        "    local_learning(X, y, X_test, y_test, config)\n",
        "    # and now the full glory of federated learning\n",
        "    print(\"################################ FEDERATED LEARNING (Linear Regression) #############################\")\n",
        "    start = time.time()\n",
        "    federated_learning(X, y, X_test, y_test, config)\n",
        "    end = time.time()\n",
        "    print(\"Time required for Federated Learning Process: \", (end-start)/(X_test.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTbScY7AlDok",
        "outputId": "10daf656-4a11-40e8-f494-d31fced839fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data\n",
            "################################ LOCAL LEARNING (Linear Regression) #############################\n",
            "Error (MSE) that each client gets on test set by training only on own local data:\n",
            "Hospital 1:\t3810.44\n",
            "Hospital 2:\t3982.58\n",
            "Hospital 3:\t3569.32\n",
            "Hospital 4:\t4144.15\n",
            "Hospital 5:\t3848.39\n",
            "Time required for Single Prediction (Local learning):  0.00011647224426269531\n",
            "################################ FEDERATED LEARNING (Linear Regression) #############################\n",
            "Running distributed gradient aggregation for 50 iterations\n",
            "Error (MSE) that each client gets after running the protocol:\n",
            "Hospital 1:\t3775.50\n",
            "Hospital 2:\t3775.50\n",
            "Hospital 3:\t3775.50\n",
            "Hospital 4:\t3775.50\n",
            "Hospital 5:\t3775.50\n",
            "Time required for Single Prediction (Local learning):  6.678104400634766e-05\n",
            "Time required for Federated Learning Process:  1.1341312265396117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part - 2: Partial Privacy Preserving SVM "
      ],
      "metadata": {
        "id": "MOOuJA-QvDJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implementation of Privacy Preserving SVM on a spam classification dataset"
      ],
      "metadata": {
        "id": "cksimD-ihNGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import gradient\n",
        "import time\n",
        "import os.path\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import phe as paillier\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Enron spam dataset hosted by https://cloudstor.aarnet.edu.au\n",
        "url = [\n",
        "    'https://cloudstor.aarnet.edu.au/plus/index.php/s/RpHZ57z2E3BTiSQ/download',\n",
        "    'https://cloudstor.aarnet.edu.au/plus/index.php/s/QVD4Xk5Cz3UVYLp/download'\n",
        "]\n",
        "\n",
        "\n",
        "def download_data():\n",
        "    \"\"\"Download two sets of Enron1 spam/ham e-mails if they are not here\n",
        "    We will use the first as trainset and the second as testset.\n",
        "    Return the path prefix to us to load the data from disk.\"\"\"\n",
        "\n",
        "    n_datasets = 2\n",
        "    for d in range(1, n_datasets + 1):\n",
        "        if not os.path.isdir('enron%d' % d):\n",
        "\n",
        "            URL = url[d-1]\n",
        "            print(\"Downloading %d/%d: %s\" % (d, n_datasets, URL))\n",
        "            folderzip = 'enron%d.zip' % d\n",
        "\n",
        "            with urlopen(URL) as remotedata:\n",
        "                with open(folderzip, 'wb') as z:\n",
        "                    z.write(remotedata.read())\n",
        "\n",
        "            with ZipFile(folderzip) as z:\n",
        "                z.extractall()\n",
        "            os.remove(folderzip)\n",
        "\n",
        "\n",
        "def preprocess_data(config):\n",
        "    \"\"\"\n",
        "    Get the Enron e-mails from disk.\n",
        "    Represent them as bag-of-words.\n",
        "    Shuffle and split train/test.\n",
        "    \"\"\"\n",
        "    print(\"Importing dataset from disk...\")\n",
        "    path = 'enron1/ham/'\n",
        "    ham1 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "            for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "    path = 'enron1/spam/'\n",
        "    spam1 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "             for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "    path = 'enron2/ham/'\n",
        "    ham2 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "            for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "    path = 'enron2/spam/'\n",
        "    spam2 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "             for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "\n",
        "    # Merge and create labels\n",
        "    emails = ham1 + spam1 + ham2 + spam2\n",
        "    y = np.array([-1] * len(ham1) + [1] * len(spam1) +\n",
        "                 [-1] * len(ham2) + [1] * len(spam2))\n",
        "\n",
        "    # Words count, keep only frequent words\n",
        "    count_vect = CountVectorizer(decode_error='replace', stop_words='english',\n",
        "                                 min_df=0.001)\n",
        "    X = count_vect.fit_transform(emails)\n",
        "    X = X.toarray()\n",
        "\n",
        "    print('Vocabulary size: %d' % X.shape[1])\n",
        "\n",
        "    # Shuffle\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    X, y = X[perm, :], y[perm]\n",
        "\n",
        "    # Split train and test\n",
        "    split = 50\n",
        "    X_train, X_test = X[-split:, :], X[:-split, :]\n",
        "    y_train, y_test = y[-split:], y[:-split]\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    print(\"Labels in trainset are {:.2f} spam : {:.2f} ham\".format(\n",
        "        np.mean(y_train == 1), np.mean(y_train == -1)))\n",
        "    \n",
        "    # Split train among multiple clients.\n",
        "    # The selection is not at random. We simulate the fact that each client\n",
        "    # sees a potentially very different sample of patients.\n",
        "    X, y = [], []\n",
        "    step = int(X_train.shape[0] / config['n_clients'])\n",
        "    for c in range(config['n_clients']):\n",
        "        X.append(X_train[step * c: step * (c + 1), :])\n",
        "        y.append(y_train[step * c: step * (c + 1)])\n",
        "\n",
        "    return X, y, X_test, y_test\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def timer():\n",
        "    \"\"\"Helper for measuring runtime\"\"\"\n",
        "\n",
        "    time0 = time.perf_counter()\n",
        "    yield\n",
        "    print('[elapsed time: %.2f s]' % (time.perf_counter() - time0))\n",
        "\n",
        "def mean_square_error(y_pred, y):\n",
        "    \"\"\" 1/m * \\sum_{i=1..m} (y_pred_i - y_i)^2 \"\"\"\n",
        "    return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "\n",
        "def encrypt_vector(public_key, x):\n",
        "    try:\n",
        "        return np.array([public_key.encrypt(i) for i in x])\n",
        "    except:\n",
        "        return public_key.encrypt(int(x))\n",
        "\n",
        "def decrypt_vector(private_key, x):\n",
        "    try:\n",
        "        return np.array([private_key.decrypt(i) for i in x])\n",
        "    except:\n",
        "         return private_key.decrypt(x)\n",
        "\n",
        "def sum_encrypted_vectors(x, y):\n",
        "    # print(type(x), type(y))\n",
        "    try:\n",
        "        if len(x) != len(y):\n",
        "            raise ValueError('Encrypted vectors must have the same size')\n",
        "        return np.array([x[i] + y[i] for i in range(len(x))])\n",
        "    except:\n",
        "        return x + y\n",
        "\n",
        "\n",
        "class Server:\n",
        "    \"\"\"Private key holder. Decrypts the average gradient\"\"\"\n",
        "\n",
        "    def __init__(self, key_length):\n",
        "         keypair = paillier.generate_paillier_keypair(n_length=key_length)\n",
        "         self.pubkey, self.privkey = keypair\n",
        "\n",
        "    def decrypt_aggregate(self, input_model, input_model_2, n_clients):\n",
        "        if input_model_2:\n",
        "            return decrypt_vector(self.privkey, input_model) / n_clients, decrypt_vector(self.privkey, input_model_2) / n_clients\n",
        "        return decrypt_vector(self.privkey, input_model) / n_clients, None\n",
        "\n",
        "\n",
        "class Client:\n",
        "    \"\"\"Runs linear regression with local data or by gradient steps,\n",
        "    where gradient can be passed in.\n",
        "\n",
        "    Using public key can encrypt locally computed gradients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name, X, y, pubkey, lr=0.01, lambda_param=0.01):\n",
        "        self.name = name\n",
        "        self.pubkey = pubkey\n",
        "        self.X, self.y = X, y\n",
        "        self.learning_rate = lr\n",
        "        self.lambda_param = lambda_param\n",
        "        n_samples, n_features = self.X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, n_iter=50):\n",
        "        # Perform gradient descent to optimize the SVM\n",
        "        for i in tqdm(range(n_iter), desc='local_learn_iters'):\n",
        "            for idx, x_i in enumerate(self.X):\n",
        "                gradient_w, gradient_b = self.compute_gradient(idx, x_i)\n",
        "                self.gradient_step(gradient_w, gradient_b)\n",
        "\n",
        "    def gradient_step(self, gradient_w, gradient_b):\n",
        "        \"\"\"Update the model with the given gradient\"\"\"\n",
        "        self.w -= self.learning_rate * gradient_w\n",
        "        if gradient_b:\n",
        "           self.b -= self.learning_rate * gradient_b\n",
        "\n",
        "    def compute_gradient(self, idx, x_i):\n",
        "        \"\"\"Compute the gradient of the current model using the training set\n",
        "        \"\"\"\n",
        "        # print(self.y[idx] * (np.dot(x_i, self.w) - self.b))\n",
        "        condition = self.y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "        # print(condition)\n",
        "        if condition:\n",
        "            gradient_w = 2 * self.lambda_param * self.w\n",
        "            return gradient_w, None\n",
        "        else:\n",
        "            gradient_w = 2 * self.lambda_param * self.w - np.dot(x_i, self.y[idx])\n",
        "            gradient_b = self.y[idx]\n",
        "            return gradient_w, gradient_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions on new data points\n",
        "        linear_output = np.dot(X, self.w) - self.b\n",
        "        return np.sign(linear_output)\n",
        "    \n",
        "\n",
        "    def encrypted_gradient(self, idx, x_i, sum_to_w=None, sum_to_b=None):\n",
        "        \"\"\"Compute and encrypt gradient.\n",
        "\n",
        "        When `sum_to` is given, sum the encrypted gradient to it, assumed\n",
        "        to be another vector of the same size\n",
        "        \"\"\"\n",
        "        g_w, g_b = self.compute_gradient(idx, x_i)\n",
        "        # print(type(g_w), type(g_b))\n",
        "        # print('   INNER CHECKPOINT 0')\n",
        "        encrypted_g_w = encrypt_vector(self.pubkey, g_w)\n",
        "        # print(\"   INNER CHECKPOINT 1\")\n",
        "        # print(g_b)\n",
        "        encrypted_g_b = None\n",
        "        if g_b:\n",
        "            encrypted_g_b = encrypt_vector(self.pubkey, g_b)\n",
        "        # print(\"   INNER CHECKPOINT 2\")\n",
        "\n",
        "        if sum_to_w is not None and sum_to_b is not None:\n",
        "            if g_b and encrypted_g_b: \n",
        "               return sum_encrypted_vectors(sum_to_w, encrypted_g_w), sum_encrypted_vectors(sum_to_b, encrypted_g_b)\n",
        "            return sum_encrypted_vectors(sum_to_w, encrypted_g_w), None\n",
        "        else:\n",
        "            if g_b and encrypted_g_b: \n",
        "                return encrypted_g_w, encrypted_g_b\n",
        "            return encrypted_g_w, None\n",
        "\n",
        "\n",
        "def federated_learning(X, y, X_test, y_test, config):\n",
        "    n_clients = config['n_clients']\n",
        "    n_iter = config['n_iter']\n",
        "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
        "\n",
        "    # Instantiate the server and generate private and public keys\n",
        "    # NOTE: using smaller keys sizes wouldn't be cryptographically safe\n",
        "    server = Server(key_length=config['key_length'])\n",
        "\n",
        "    # Instantiate the clients.\n",
        "    # Each client gets the public key at creation and its own local dataset\n",
        "    clients = []\n",
        "    for i in range(n_clients):\n",
        "        clients.append(Client(names[i], X[i], y[i], server.pubkey))\n",
        "\n",
        "    # The federated learning with gradient descent\n",
        "    print('Running distributed gradient aggregation for {:d} iterations'.format(n_iter))\n",
        "\n",
        "    for i in tqdm(range(n_iter), desc='fed_learn_iters'):\n",
        "          # Compute gradients, encrypt and aggregate\n",
        "          for idx in tqdm(range(X[0].shape[0]), desc='fed_learn_data_iter'):\n",
        "              x_i = X[0][idx, :]\n",
        "              # print('CHECKPOINT 0')\n",
        "              encrypt_aggr_w, encrypt_aggr_b = clients[0].encrypted_gradient(idx, x_i, sum_to_w=None, sum_to_b=None)\n",
        "              # print('CHECKPOINT 1')\n",
        "              for i in range(1, config['n_clients']):\n",
        "                  c = clients[i]\n",
        "                  x_i = X[i][idx, :]\n",
        "                  encrypt_aggr_w, encrypt_aggr_b = c.encrypted_gradient(idx, x_i, sum_to_w=encrypt_aggr_w, sum_to_b=encrypt_aggr_b)\n",
        "              # print('CHECKPOINT 2')\n",
        "              # Send aggregate to server and decrypt it\n",
        "              aggr_w, aggr_b = server.decrypt_aggregate(encrypt_aggr_w, encrypt_aggr_b, n_clients)\n",
        "              # print('CHECKPOINT 3')\n",
        "              # Take gradient steps\n",
        "              for c in clients:\n",
        "                  c.gradient_step(aggr_w, aggr_b)\n",
        "\n",
        "    print('Accuracy that each client gets after running the protocol:')\n",
        "    start = time.time()\n",
        "    for c in clients:\n",
        "        y_pred = c.predict(X_test)\n",
        "        hinge_l = accuracy_score(y_test, y_pred)\n",
        "        print('{:s}:\\t{:.2f}'.format(c.name, hinge_l))\n",
        "    end = time.time()\n",
        "    print(\"Time required for Single Prediction (Federated learning): \", (end-start)/(X_test.shape[0]))\n",
        "\n",
        "\n",
        "def local_learning(X, y, X_test, y_test, config):\n",
        "    n_clients = config['n_clients']\n",
        "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
        "\n",
        "    # Instantiate the clients.\n",
        "    # Each client gets the public key at creation and its own local dataset\n",
        "    clients = []\n",
        "    for i in tqdm(range(n_clients), desc='local_learn_client_number'):\n",
        "        clients.append(Client(names[i], X[i], y[i], None))\n",
        "\n",
        "    # Each client trains a linear regressor on its own data\n",
        "    print('Accuracy that each client gets on test set by '\n",
        "          'training only on own local data:')\n",
        "    start = time.time()\n",
        "    for c in clients:\n",
        "        c.fit(config['n_iter'])\n",
        "        y_pred = c.predict(X_test)\n",
        "        hinge_l = accuracy_score(y_test, y_pred)\n",
        "        print('{:s}:\\t{:.2f}'.format(c.name, hinge_l))\n",
        "    end = time.time()\n",
        "    print(\"Time required for Single Prediction (Local learning): \", (end-start)/(X_test.shape[0]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'n_clients': 5,\n",
        "        'key_length': 128,\n",
        "        'n_iter': 50,\n",
        "        'eta': 1.5,\n",
        "    }\n",
        "    # load data, train/test split and split training data between clients\n",
        "    download_data()\n",
        "    X, y, X_test, y_test = preprocess_data(config)\n",
        "    print(X[0].shape, y[0].shape, X_test.shape, y_test.shape)\n",
        "    # first each hospital learns a model on its respective dataset for comparison.\n",
        "    print(\"######################## LOCAL LEARNING (SVM) ###############################\")\n",
        "    local_learning(X, y, X_test, y_test, config)\n",
        "    # and now the full glory of federated learning\n",
        "    print(\"######################## FEDERATED LEARNING (SVM) ###############################\")\n",
        "    start = time.time()\n",
        "    federated_learning(X, y, X_test, y_test, config)\n",
        "    end = time.time()\n",
        "    print(\"Time required for Federated Learning Process: \", (end-start)/(X_test.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cImPzqCvd4Y",
        "outputId": "1f1dbdba-a087-4c6c-e314-6ba4ba90d6d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing dataset from disk...\n",
            "Vocabulary size: 7997\n",
            "(50, 7997) (10979, 7997)\n",
            "Labels in trainset are 0.22 spam : 0.78 ham\n",
            "(10, 7997) (10,) (10979, 7997) (10979,)\n",
            "######################## LOCAL LEARNING (SVM) ###############################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "local_learn_client_number: 100%|██████████| 5/5 [00:00<00:00, 22453.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy that each client gets on test set by training only on own local data:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "local_learn_iters: 100%|██████████| 50/50 [00:00<00:00, 2724.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hospital 1:\t0.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "local_learn_iters: 100%|██████████| 50/50 [00:00<00:00, 1847.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hospital 2:\t0.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "local_learn_iters: 100%|██████████| 50/50 [00:00<00:00, 2366.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hospital 3:\t0.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "local_learn_iters: 100%|██████████| 50/50 [00:00<00:00, 2039.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hospital 4:\t0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "local_learn_iters: 100%|██████████| 50/50 [00:00<00:00, 2118.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hospital 5:\t0.74\n",
            "Time required for Single Prediction (Local learning):  0.00014709266702902317\n",
            "######################## FEDERATED LEARNING (SVM) ###############################\n",
            "Running distributed gradient aggregation for 50 iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fed_learn_iters:   0%|          | 0/50 [00:00<?, ?it/s]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:49,  5.55s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:13<00:53,  6.73s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:20<00:49,  7.02s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:26<00:40,  6.70s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:32<00:32,  6.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:38<00:25,  6.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:45<00:19,  6.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:51<00:12,  6.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:57<00:06,  6.29s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [01:03<00:00,  6.32s/it]\n",
            "fed_learn_iters:   2%|▏         | 1/50 [01:03<51:39, 63.26s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:52,  5.84s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:11<00:44,  5.58s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:22<00:33,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:26,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:49<00:05,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.50s/it]\n",
            "fed_learn_iters:   4%|▍         | 2/50 [01:58<46:42, 58.38s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:47,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.46s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n",
            "fed_learn_iters:   6%|▌         | 3/50 [02:52<44:07, 56.33s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.63s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:11<00:44,  5.57s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.46s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:22<00:33,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:33<00:22,  5.66s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:44<00:11,  5.54s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:49<00:05,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:55<00:00,  5.50s/it]\n",
            "fed_learn_iters:   8%|▊         | 4/50 [03:47<42:47, 55.82s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:48,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.50s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.42s/it]\n",
            "fed_learn_iters:  10%|█         | 5/50 [04:41<41:26, 55.25s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:49,  5.55s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
            "fed_learn_iters:  12%|█▏        | 6/50 [05:35<40:09, 54.77s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:48,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.50s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.41s/it]\n",
            "fed_learn_iters:  14%|█▍        | 7/50 [06:29<39:06, 54.57s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.99s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:44,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.46s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.46s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:11,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:56<00:00,  5.66s/it]\n",
            "fed_learn_iters:  16%|█▌        | 8/50 [07:25<38:38, 55.20s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:06<00:55,  6.12s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:11<00:47,  5.97s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:17<00:39,  5.60s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:22<00:33,  5.52s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:28<00:27,  5.57s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:33<00:21,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:39<00:16,  5.58s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:44<00:10,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:50<00:05,  5.60s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:55<00:00,  5.57s/it]\n",
            "fed_learn_iters:  18%|█▊        | 9/50 [08:21<37:50, 55.37s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:51,  5.76s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:39,  5.66s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:26,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:11,  5.52s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:49<00:05,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.48s/it]\n",
            "fed_learn_iters:  20%|██        | 10/50 [09:16<36:47, 55.19s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:48,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:11<00:44,  5.61s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
            "fed_learn_iters:  22%|██▏       | 11/50 [10:10<35:36, 54.78s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.61s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.40s/it]\n",
            "fed_learn_iters:  24%|██▍       | 12/50 [11:04<34:33, 54.55s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:47,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.33s/it]\n",
            "fed_learn_iters:  26%|██▌       | 13/50 [11:57<33:24, 54.19s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.08s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.23s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.34s/it]\n",
            "fed_learn_iters:  28%|██▊       | 14/50 [12:51<32:22, 53.96s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:49,  5.52s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:41,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:27,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.37s/it]\n",
            "fed_learn_iters:  30%|███       | 15/50 [13:44<31:26, 53.90s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.99s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.36s/it]\n",
            "fed_learn_iters:  32%|███▏      | 16/50 [14:38<30:29, 53.80s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:49,  5.52s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.23s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
            "fed_learn_iters:  34%|███▍      | 17/50 [15:32<29:35, 53.81s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.03s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.40s/it]\n",
            "fed_learn_iters:  36%|███▌      | 18/50 [16:26<28:43, 53.86s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:52,  5.80s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:11<00:43,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:26,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.50s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.47s/it]\n",
            "fed_learn_iters:  38%|███▊      | 19/50 [17:21<27:57, 54.12s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:46,  5.20s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.53s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.53s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.43s/it]\n",
            "fed_learn_iters:  40%|████      | 20/50 [18:15<27:04, 54.17s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:51,  5.76s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:22,  5.55s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.46s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.45s/it]\n",
            "fed_learn_iters:  42%|████▏     | 21/50 [19:09<26:14, 54.28s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:44,  5.00s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:40,  5.12s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.23s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:27,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.36s/it]\n",
            "fed_learn_iters:  44%|████▍     | 22/50 [20:03<25:14, 54.08s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:47,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:26,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:22,  5.50s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n",
            "fed_learn_iters:  46%|████▌     | 23/50 [20:57<24:19, 54.04s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.59s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.42s/it]\n",
            "fed_learn_iters:  48%|████▊     | 24/50 [21:51<23:26, 54.10s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:47,  5.23s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.49s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n",
            "fed_learn_iters:  50%|█████     | 25/50 [22:45<22:31, 54.06s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:52,  5.85s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:38<00:16,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.44s/it]\n",
            "fed_learn_iters:  52%|█████▏    | 26/50 [23:40<21:40, 54.17s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.03s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.37s/it]\n",
            "fed_learn_iters:  54%|█████▍    | 27/50 [24:33<20:42, 54.03s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.67s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.55s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n",
            "fed_learn_iters:  56%|█████▌    | 28/50 [25:27<19:47, 54.00s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.98s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.18s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.34s/it]\n",
            "fed_learn_iters:  58%|█████▊    | 29/50 [26:21<18:50, 53.83s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:52,  5.85s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:36,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.43s/it]\n",
            "fed_learn_iters:  60%|██████    | 30/50 [27:15<17:59, 53.98s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:46,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.52s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.35s/it]\n",
            "fed_learn_iters:  62%|██████▏   | 31/50 [28:08<17:02, 53.83s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.97s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.31s/it]\n",
            "fed_learn_iters:  64%|██████▍   | 32/50 [29:01<16:04, 53.60s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.59s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:41,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:27,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.35s/it]\n",
            "fed_learn_iters:  66%|██████▌   | 33/50 [29:55<15:10, 53.57s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.92s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.20s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:20,  5.21s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:36<00:15,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:41<00:10,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.25s/it]\n",
            "fed_learn_iters:  68%|██████▊   | 34/50 [30:47<14:11, 53.24s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.60s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.14s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:36<00:15,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.27s/it]\n",
            "fed_learn_iters:  70%|███████   | 35/50 [31:40<13:16, 53.08s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:48,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:41,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.35s/it]\n",
            "fed_learn_iters:  72%|███████▏  | 36/50 [32:34<12:25, 53.22s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.05s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.19s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.23s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:20,  5.21s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:36<00:15,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:41<00:10,  5.15s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.24s/it]\n",
            "fed_learn_iters:  74%|███████▍  | 37/50 [33:26<11:28, 52.97s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:48,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:41,  5.21s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:25,  5.15s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:36<00:15,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.29s/it]\n",
            "fed_learn_iters:  76%|███████▌  | 38/50 [34:19<10:35, 52.96s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:47,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.17s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.29s/it]\n",
            "fed_learn_iters:  78%|███████▊  | 39/50 [35:12<09:42, 52.94s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.02s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.28s/it]\n",
            "fed_learn_iters:  80%|████████  | 40/50 [36:05<08:48, 52.89s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:48,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:40,  5.11s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.21s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.24s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:36<00:15,  5.15s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:41<00:10,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:46<00:05,  5.18s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.27s/it]\n",
            "fed_learn_iters:  82%|████████▏ | 41/50 [36:57<07:55, 52.83s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.91s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:30,  5.15s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.28s/it]\n",
            "fed_learn_iters:  84%|████████▍ | 42/50 [37:50<07:02, 52.81s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:46,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.41s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.29s/it]\n",
            "fed_learn_iters:  86%|████████▌ | 43/50 [38:43<06:09, 52.83s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:50,  5.59s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:31,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.44s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.37s/it]\n",
            "fed_learn_iters:  88%|████████▊ | 44/50 [39:37<05:18, 53.09s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.01s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.32s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:20,  5.17s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:36<00:15,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:41<00:10,  5.17s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.24s/it]\n",
            "fed_learn_iters:  90%|█████████ | 45/50 [40:29<04:24, 52.90s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:49,  5.55s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:37,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.34s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:25,  5.20s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:15,  5.25s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.22s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.32s/it]\n",
            "fed_learn_iters:  92%|█████████▏| 46/50 [41:22<03:31, 53.00s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.01s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:09<00:39,  4.99s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.33s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:20<00:31,  5.19s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.35s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:31<00:21,  5.26s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.38s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.27s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:47<00:05,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:52<00:00,  5.28s/it]\n",
            "fed_learn_iters:  94%|█████████▍| 47/50 [42:15<02:38, 52.94s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:04<00:44,  4.92s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:43,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:36,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:26,  5.28s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:42<00:10,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.42s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.36s/it]\n",
            "fed_learn_iters:  96%|█████████▌| 48/50 [43:09<01:46, 53.13s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:49,  5.52s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.29s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:16<00:38,  5.48s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:32,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:27<00:27,  5.53s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.37s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.39s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.31s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:54<00:00,  5.40s/it]\n",
            "fed_learn_iters:  98%|█████████▊| 49/50 [44:03<00:53, 53.40s/it]\n",
            "fed_learn_data_iter:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "fed_learn_data_iter:  10%|█         | 1/10 [00:05<00:45,  5.06s/it]\u001b[A\n",
            "fed_learn_data_iter:  20%|██        | 2/10 [00:10<00:42,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  30%|███       | 3/10 [00:15<00:37,  5.30s/it]\u001b[A\n",
            "fed_learn_data_iter:  40%|████      | 4/10 [00:21<00:33,  5.51s/it]\u001b[A\n",
            "fed_learn_data_iter:  50%|█████     | 5/10 [00:26<00:27,  5.43s/it]\u001b[A\n",
            "fed_learn_data_iter:  60%|██████    | 6/10 [00:32<00:21,  5.40s/it]\u001b[A\n",
            "fed_learn_data_iter:  70%|███████   | 7/10 [00:37<00:16,  5.45s/it]\u001b[A\n",
            "fed_learn_data_iter:  80%|████████  | 8/10 [00:43<00:10,  5.36s/it]\u001b[A\n",
            "fed_learn_data_iter:  90%|█████████ | 9/10 [00:48<00:05,  5.47s/it]\u001b[A\n",
            "fed_learn_data_iter: 100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n",
            "fed_learn_iters: 100%|██████████| 50/50 [44:57<00:00, 53.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy that each client gets after running the protocol:\n",
            "Hospital 1:\t0.83\n",
            "Hospital 2:\t0.83\n",
            "Hospital 3:\t0.83\n",
            "Hospital 4:\t0.83\n",
            "Hospital 5:\t0.83\n",
            "Time required for Single Prediction (Federated learning):  0.00010633208037443375\n",
            "Time required for Federated Learning Process:  0.24577568445779857\n"
          ]
        }
      ]
    }
  ]
}